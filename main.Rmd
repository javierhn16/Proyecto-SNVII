---
title: "main"
output: html_document
---

# Lectura y librerías
```{r}
pacman::p_load(
  tidyverse,
  lubridate, 
  ggplot2, 
  scales,
  fitdistrplus,
  copula,
  dplyr,
  plotly,
  ChainLadder,
  stats,
  zoo,
  pbapply,
  pracma
)
```

```{r,message=FALSE}
data <- read.csv("data/ausautoBI8999.csv", stringsAsFactors = FALSE) %>%
  arrange(AccMth) %>%
  mutate(
    AccDate = as.Date(AccDate),
    ReportDate = as.Date(ReportDate),
    FinDate = as.Date(FinDate),
    AccMth    = as.yearmon(AccMth, "%Y-%m"),
    ReportMth = as.yearmon(ReportMth, "%Y-%m"),
    FinMth    = as.yearmon(FinMth, "%Y-%m"),
    delay = as.numeric((ReportMth - AccMth)),
    duration = as.numeric(FinMth - AccMth)
  ) %>% 
  filter(AccDate > as.Date("1992-12-31")) %>%
  filter(AccDate < as.Date("1999-1-1")) %>%
  filter(ReportDate < as.Date("1999-1-1")) %>%
  filter(!is.na(delay), !is.na(duration), delay >= 0, duration >= 0) %>%
  dplyr::select(-c(4:6,8:14))
```

# AED
```{r}
fig <- ggplot(data, aes(x = log(AggClaim))) +
  geom_density(color = "lightblue", linewidth = 1.3, adjust = 1.2, fill = "lightblue") +
  geom_histogram(aes(y = ..density..), 
                 fill = "#0073C2FF", color = "white", bins = 50, alpha = 0.6) +
  labs(
    x = "Monto del reclamo (escala log)",
    y = "Densidad"
  ) +
  theme_minimal()
ggsave("res/densidad.pdf", fig, width = 8, height = 4)
```

```{r, message=FALSE}
fig <- data %>%
  mutate(month = floor_date(AccDate, "month")) %>%
  group_by(month) %>%
  summarise(Reclamos = n()) %>%
  ggplot(aes(x = month, y = Reclamos)) +
  geom_line(color = "#E69F00", linewidth = 1) +
  geom_point(color = "#F17F00", size = 1.5) +
  geom_smooth(method = "loess", se = FALSE, color = "#0072B2") +  
  labs(
    x = "Fecha del accidente (mes)",
    y = "Número de reclamos"
  ) +
  theme_minimal()
ggsave("res/evol.pdf", fig, width = 8, height = 4)
```

```{r}
fig <- ggplot(data, aes(x = log(AggClaim), y = OpTime)) +
  geom_density_2d_filled(contour_var = "density", alpha = 0.85) +
  scale_fill_manual(
    values = colorRampPalette(c("#02102a", "#3182bd", "#fef0d9", "gray")[4:1])(13)
  ) +
  labs(
    x = "Monto agregado del reclamo (escala log)",
    y = "Tiempo operativo (OpTime)",
    fill = "Densidad"
  ) +
  theme_minimal()
ggsave("res/calor.pdf", fig, width = 8, height = 4)
```

# Chain Ladder
```{r}
datos_sli <- data %>%
  mutate(
    ocurrencia = year(AccDate),
    reporte = year(ReportDate),
    k  = pmax(0, reporte - ocurrencia)
  ) %>%
  group_by(ocurrencia, k) %>%
  summarise(inc = sum(AggClaim, na.rm = TRUE), .groups = "drop")

  triangulo <- pivot_wider(datos_sli, names_from = k, values_from = inc, values_fill = 0) %>%
    arrange(ocurrencia)
  triangulo <- data.frame(triangulo)
  rownames(triangulo) <- triangulo[, 1]  
  triangulo <- triangulo[, -1]           
  colnames(triangulo) <- 0:max(datos_sli$k, na.rm = TRUE)
```

```{r}
cumtria <- function(triangulo) {
  n <- nrow(triangulo)
  mask <- upper.tri(matrix(1, n, n), diag = T)[, n:1]
  acum <- t(apply(triangulo, 1, cumsum)) * mask
  acum[!mask] <- 0
  acum <- data.frame(acum)
  colnames(acum) <- 0:(n - 1)
  return(acum)
}
```

```{r}
tria <- cumtria(triangulo)
n <- nrow(tria)
mask <- upper.tri(matrix(TRUE, n, n), diag = TRUE)[, n:1]  
tria[!mask] <- NA 
tria <- as.triangle(as.matrix(tria))
CL <- MackChainLadder(tria)
```

```{r}
(tria <- CL$FullTriangle)
```

```{r}
CL$f
```


```{r}
# Reserva
sum(tria[,6]-diag(tria))
```

# Estimación por cópulas

```{r}
# 1) Preprocessing 
prepare_times <- function(data, date_origin = NULL) {
  if (!inherits(data$AccDate, "Date")) data$AccDate <- as.Date(data$AccDate)
  if (!inherits(data$ReportDate, "Date")) data$ReportDate <- as.Date(data$ReportDate)
  if (is.null(date_origin)) date_origin <- min(data$AccDate, na.rm = TRUE)
  data$Ti <- as.numeric(difftime(data$AccDate, date_origin, units = "days"))
  data$Si <- as.numeric(difftime(data$ReportDate, date_origin, units = "days"))
  stopifnot(all(data$Si >= data$Ti, na.rm = TRUE))
  data <- data[order(data$Ti), , drop = FALSE]
  data$i <- seq_len(nrow(data))
  data$Tprime <- c(data$Ti[1], diff(data$Ti))
  data$Wi <- data$Si - data$Ti
  return(list(data = data, origin = date_origin))
}

# 2) Marginals & Clayton
f_exp <- function(x, lambda) {
  y <- lambda * exp(-lambda * pmax(x, 0))
  y[x < 0] <- 0
  y
}

F_exp <- function(x, lambda) {
  y <- 1 - exp(-lambda * pmax(x, 0))
  y[x < 0] <- 0
  y
}

c_clayton <- function(u, v, theta) {
  if (theta <= 0) return(1.0)
  u <- pmin(pmax(u, 1e-12), 1 - 1e-12)
  v <- pmin(pmax(v, 1e-12), 1 - 1e-12)
  term <- (u^(-theta) + v^(-theta) - 1)
  (theta + 1) * (u * v)^(-theta - 1) * term^(-2 - 1 / theta)
}

# Gauss–Legendre 64-pt quadrature
quad64 <- local({
  x <- c(-0.960289856, -0.796666477, -0.525532410, -0.183434642,
         0.183434642, 0.525532410, 0.796666477, 0.960289856)
  w <- c(0.101228536, 0.222381034, 0.313706646, 0.362683783,
         0.362683783, 0.313706646, 0.222381034, 0.101228536)
  list(x = x, w = w)
})

# Optimized joint density
f_TiSi <- function(t, s, i, lambda1, lambda2, theta) {
  if (s < t) return(0)
  
  # i == 1: closed form
  if (i == 1) {
    tau <- t
    w <- s
    val <- f_exp(tau, lambda1) * f_exp(w, lambda2) *
      c_clayton(F_exp(tau, lambda1), F_exp(w, lambda2), theta)
    return(max(val, 1e-300))
  }
  
  # Integration via 64-point Gauss-Legendre
  a <- 0; b <- t
  xs <- ((b - a) / 2) * quad64$x + (b + a) / 2
  ws <- quad64$w
  
  tau <- t - xs
  w <- s - xs
  mask <- (tau >= 0) & (w >= 0)
  if (!any(mask)) return(0)
  
  # vectorized integrand
  f_tau <- f_exp(tau[mask], lambda1)
  f_w   <- f_exp(w[mask], lambda2)
  u1 <- F_exp(tau[mask], lambda1)
  v1 <- F_exp(w[mask], lambda2)
  copd <- c_clayton(u1, v1, theta)
  f_Tim1 <- dgamma(xs[mask], shape = i - 1, rate = lambda1)
  
  y <- f_tau * f_w * copd * f_Tim1
  val <- sum(ws[mask] * y) * (b - a) / 2
  max(val, 1e-300)
}

# 4) Log-likelihood (vectorized)
loglik_params <- function(params, data) {
  lambda1 <- params[1]; lambda2 <- params[2]; theta <- params[3]
  if (lambda1 <= 0 || lambda2 <= 0 || theta < 0) return(1e20)
  
  vals <- vapply(seq_len(nrow(data)), function(idx) {
    f_TiSi(data$Ti[idx], data$Si[idx], data$i[idx],
           lambda1, lambda2, theta)
  }, numeric(1))
  
  if (any(vals <= 0 | !is.finite(vals))) return(1e20)
  -sum(log(vals))
}

# 5) Estimation (MLE)
estimate_params <- function(data, init = NULL,
                            lower = c(1e-6, 1e-6, 0),
                            upper = c(Inf, Inf, Inf)) {
  if (is.null(init)) {
    lambda1_init <- 1 / mean(pmax(data$Tprime, 1e-6))
    lambda2_init <- 1 / mean(pmax(data$Wi, 1e-6))
    theta_init <- 0.5
    init <- c(lambda1_init, lambda2_init, theta_init)
  }
  opt <- optim(par = init,
               fn = loglik_params,
               data = data,
               method = "L-BFGS-B",
               lower = lower,
               upper = upper,
               control = list(maxit = 200))
  est <- opt$par
  names(est) <- c("lambda1", "lambda2", "theta")
  list(est = est, optim = opt)
}

```


```{r}
P_Ti_in_interval <- function(i, interval, lambda1) {
  pgamma(interval[2], shape = i, rate = lambda1) - pgamma(interval[1], shape = i, rate = lambda1)
}

# --- Numerador optimizado y "limpio" ---
P_Ti_Si_in_intervals_num <- function(i, interval_t, interval_s, lambda1, lambda2, theta,
                                     grid_t = 20, grid_s = 20) {
  ts <- seq(interval_t[1], interval_t[2], length.out = grid_t)
  ss <- seq(interval_s[1], interval_s[2], length.out = grid_s)
  dx <- (interval_t[2] - interval_t[1]) / (grid_t - 1)
  dy <- (interval_s[2] - interval_s[1]) / (grid_s - 1)
  use_gamma <- (i > 1)

  log_vals <- matrix(-Inf, nrow = grid_t, ncol = grid_s)

  for (ii in seq_along(ts)) {
    t <- ts[ii]
    us <- seq(0, t, length.out = 25)  # cuadratura interna más ligera
    for (jj in seq_along(ss)) {
      s <- ss[jj]
      if (s < t) next

      tau <- t - us
      w   <- s - us
      valid <- (tau >= 0 & w >= 0)
      if (!any(valid)) next

      tau <- tau[valid]
      w   <- w[valid]
      u   <- us[valid]

      # Componentes en log
      log_f_tau <- log(lambda1) - lambda1 * tau
      log_f_w   <- log(lambda2) - lambda2 * w
      u1 <- 1 - exp(-lambda1 * tau)
      v1 <- 1 - exp(-lambda2 * w)
      cop <- c_clayton(u1, v1, theta)
      log_cop <- ifelse(cop > 0, log(cop), -Inf)
      log_f_Tim1 <- if (use_gamma) dgamma(u, shape = i - 1, rate = lambda1, log = TRUE) else 0

      log_y <- log_f_tau + log_f_w + log_cop + log_f_Tim1

      # corte temprano si todo es -Inf
      if (all(!is.finite(log_y))) next

      # log-sum-exp + trapecio
      max_log <- suppressWarnings(max(log_y[is.finite(log_y)], na.rm = TRUE))
      if (!is.finite(max_log)) next
      val <- exp(max_log) * trapz(u, exp(log_y - max_log))
      log_vals[ii, jj] <- log(val)
    }
  }

  finite_vals <- log_vals[is.finite(log_vals)]
  if (length(finite_vals) == 0) return(1e-300)

  max_outer <- max(finite_vals)
  total <- exp(max_outer) * sum(exp(log_vals - max_outer), na.rm = TRUE) * dx * dy
  return(max(total, 1e-300))
}

# --- Cálculo de probabilidades ---
compute_delay_probs <- function(data, params, years, max_lag = 5, grid_t = 20, grid_s = 20) {
  lambda1 <- params[1]; lambda2 <- params[2]; theta <- params[3]
  n <- nrow(data)
  
  # Precompute límites de año (una sola vez)
  year_bounds <- as.Date(paste0(years, "-01-01"))
  origin <- min(data$AccDate)
  bounds_days <- as.numeric(difftime(year_bounds, origin, units = "days"))
  bounds_days <- c(bounds_days, tail(bounds_days, 1) + 365)
  
  # Precompute intervalos
  year_intervals <- lapply(seq_along(years), function(j) {
    c(bounds_days[j], bounds_days[j + 1])
  })
  
  # Progreso sobre índices, preservando tipos originales
  probs <- pbapply::pbmapply(function(k) {
    occ_year <- as.integer(format(data$AccDate[k], "%Y"))
    j_idx <- which(years == occ_year)
    if (length(j_idx) == 0) return(rep(0, max_lag + 1))
    
    i_val <- data$i[k]
    interval_t <- year_intervals[[j_idx]]
    den <- P_Ti_in_interval(i_val, interval_t, lambda1)
    if (den <= 0) return(rep(0, max_lag + 1))
    
    p_row <- numeric(max_lag + 1)
    for (l in 0:max_lag) {
      if ((j_idx + l) > length(years)) {
        p_row[l + 1] <- 0
      } else {
        interval_s <- year_intervals[[j_idx + l]]
        num <- P_Ti_Si_in_intervals_num(i_val, interval_t, interval_s,
                                        lambda1, lambda2, theta,
                                        grid_t = grid_t, grid_s = grid_s)
        p_row[l + 1] <- num / den
      }
    }
    return(p_row)
  }, k = seq_len(n), SIMPLIFY = FALSE)
  
  # Convertir lista a matriz
  probs <- do.call(rbind, probs)
  colnames(probs) <- paste0("lag_", 0:max_lag)
  return(probs)
}
```


```{r}
# 7) Construcción del triángulo

build_triangle_expected <- function(data, probs, years, max_lag = 5) {
  origin_years <- years
  n_years <- length(origin_years)
  triangle <- matrix(0, nrow = n_years, ncol = max_lag + 1)
  colnames(triangle) <- as.character(origin_years)
  rownames(triangle) <- paste0("dev_", 0:max_lag)

  occ_years <- as.integer(format(data$AccDate, "%Y"))
  year_idx <- match(occ_years, origin_years)

  for (l in 0:max_lag) {
    triangle[, l + 1] <- tapply(probs[, l + 1], year_idx, sum, default = 0)
  }

  return(t(triangle))
}

```

```{r}
prep <- prepare_times(data)
data_p <- prep$data
res_mle <- estimate_params(data_p)
params <- res_mle$est
print(params)
```

```{r}
years <- sort(unique(as.integer(format(data_p$AccDate, "%Y"))))
probs <- compute_delay_probs(data_p, params, years, max_lag = 6)
```


```{r}
# 4) Construye triángulo esperado
tri_expected <- build_triangle_expected(data_p, probs, years, max_lag = 5)
print(tri_expected)
```

```{r}
tri_expected <- cumtria(tri_expected)
n <- nrow(tri_expected)
mask <- upper.tri(matrix(TRUE, n, n), diag = TRUE)[, n:1]  
tri_expected[!mask] <- NA 
tri_expected <- as.triangle(as.matrix(tri_expected))
fds <- MackChainLadder(tri_expected)$f
```

































