---
title: "main"
output: html_document
---

# Lectura y librerías
```{r}
pacman::p_load(
  tidyverse,
  lubridate, 
  ggplot2, 
  scales,
  fitdistrplus,
  copula,
  dplyr,
  plotly,
  ChainLadder,
  stats,
  zoo
)
```

```{r,message=FALSE}
data <- read.csv("data/ausautoBI8999.csv", stringsAsFactors = FALSE) %>%
  arrange(AccMth) %>%
  mutate(
    AccDate = as.Date(AccDate),
    ReportDate = as.Date(ReportDate),
    FinDate = as.Date(FinDate),
    AccMth    = as.yearmon(AccMth, "%Y-%m"),
    ReportMth = as.yearmon(ReportMth, "%Y-%m"),
    FinMth    = as.yearmon(FinMth, "%Y-%m"),
    delay = as.numeric((ReportMth - AccMth)),
    duration = as.numeric(FinMth - AccMth)
  ) %>% 
  filter(AccDate > as.Date("1992-12-31")) %>%
  filter(AccDate < as.Date("1999-1-1")) %>%
  filter(ReportDate < as.Date("1999-1-1")) %>%
  filter(!is.na(delay), !is.na(duration), delay >= 0, duration >= 0) %>%
  dplyr::select(-c(4:6,8:14))
```

# AED
```{r}
fig <- ggplot(data, aes(x = log(AggClaim))) +
  geom_density(color = "lightblue", linewidth = 1.3, adjust = 1.2, fill = "lightblue") +
  geom_histogram(aes(y = ..density..), 
                 fill = "#0073C2FF", color = "white", bins = 50, alpha = 0.6) +
  labs(
    x = "Monto del reclamo (escala log)",
    y = "Densidad"
  ) +
  theme_minimal()
ggsave("res/densidad.pdf", fig, width = 8, height = 4)
```

```{r, message=FALSE}
fig <- data %>%
  mutate(month = floor_date(AccDate, "month")) %>%
  group_by(month) %>%
  summarise(Reclamos = n()) %>%
  ggplot(aes(x = month, y = Reclamos)) +
  geom_line(color = "#E69F00", linewidth = 1) +
  geom_point(color = "#F17F00", size = 1.5) +
  geom_smooth(method = "loess", se = FALSE, color = "#0072B2") +  
  labs(
    x = "Fecha del accidente (mes)",
    y = "Número de reclamos"
  ) +
  theme_minimal()
ggsave("res/evol.pdf", fig, width = 8, height = 4)
```

```{r}
fig <- ggplot(data, aes(x = log(AggClaim), y = OpTime)) +
  geom_density_2d_filled(contour_var = "density", alpha = 0.85) +
  scale_fill_manual(
    values = colorRampPalette(c("#02102a", "#3182bd", "#fef0d9", "gray")[4:1])(13)
  ) +
  labs(
    x = "Monto agregado del reclamo (escala log)",
    y = "Tiempo operativo (OpTime)",
    fill = "Densidad"
  ) +
  theme_minimal()
ggsave("res/calor.pdf", fig, width = 8, height = 4)
```

# Chain Ladder
```{r}
datos_sli <- data %>%
  mutate(
    ocurrencia = year(AccDate),
    reporte = year(ReportDate),
    k  = pmax(0, reporte - ocurrencia)
  ) %>%
  group_by(ocurrencia, k) %>%
  summarise(inc = sum(AggClaim, na.rm = TRUE), .groups = "drop")

  triangulo <- pivot_wider(datos_sli, names_from = k, values_from = inc, values_fill = 0) %>%
    arrange(ocurrencia)
  triangulo <- data.frame(triangulo)
  rownames(triangulo) <- triangulo[, 1]  
  triangulo <- triangulo[, -1]           
  colnames(triangulo) <- 0:max(datos_sli$k, na.rm = TRUE)
```

```{r}
cumtria <- function(triangulo) {
  n <- nrow(triangulo)
  mask <- upper.tri(matrix(1, n, n), diag = T)[, n:1]
  acum <- t(apply(triangulo, 1, cumsum)) * mask
  acum[!mask] <- 0
  acum <- data.frame(acum)
  colnames(acum) <- 0:(n - 1)
  return(acum)
}
```

```{r}
tria <- cumtria(triangulo)
n <- nrow(tria)
mask <- upper.tri(matrix(TRUE, n, n), diag = TRUE)[, n:1]  
tria[!mask] <- NA 
tria <- as.triangle(as.matrix(tria))
CL <- MackChainLadder(tria)
```

```{r}
(tria <- CL$FullTriangle)
```

```{r}
CL$f
```


```{r}
# Reserva
sum(tria[,6]-diag(tria))
```

# Estimación por cópulas

```{r}
# 1) Preprocessing
prepare_times <- function(data, date_origin = NULL) {
  if (!inherits(data$AccDate, "Date")) data$AccDate <- as.Date(data$AccDate)
  if (!inherits(data$ReportDate, "Date")) data$ReportDate <- as.Date(data$ReportDate)
  if (is.null(date_origin)) date_origin <- min(data$AccDate, na.rm = TRUE)
  data$Ti <- as.numeric(difftime(data$AccDate, date_origin, units = "days"))
  data$Si <- as.numeric(difftime(data$ReportDate, date_origin, units = "days"))
  stopifnot(all(data$Si >= data$Ti, na.rm = TRUE))
  # Order by Ti and assign index i
  data <- data[order(data$Ti), , drop = FALSE]
  data$i <- seq_len(nrow(data))
  # Inter-arrival times T'_i
  data$Tprime <- c(data$Ti[1], diff(data$Ti))
  # Waiting times W_i
  data$Wi <- data$Si - data$Ti
  return(list(data = data, origin = date_origin))
}

# 2) Marginals & Clayton 
# Exponential PDF and CDF for T' and W (rate parameterization)
f_exp <- function(x, lambda) ifelse(x >= 0, lambda * exp(-lambda * x), 0)
F_exp <- function(x, lambda) ifelse(x >= 0, 1 - exp(-lambda * x), 0)

# Clayton copula density c(u,v; theta)
# c(u,v) = (theta + 1) * (u*v)^(-theta - 1) * (u^{-theta} + v^{-theta} - 1)^{-2 - 1/theta}
# valid for theta > 0. For theta == 0, c = 1 (independence) -- we treat separately.
c_clayton <- function(u, v, theta) {
  if (theta <= 0) return(1.0)
  u <- pmin(pmax(u, 1e-12), 1 - 1e-12)
  v <- pmin(pmax(v, 1e-12), 1 - 1e-12)
  term <- (u^(-theta) + v^(-theta) - 1)
  dens <- (theta + 1) * (u * v)^(-theta - 1) * term^(-2 - 1 / theta)
  return(dens)
}

#  3) Joint density f_{T_i, S_i}(t,s) 
# For a given observed (t, s) and event index i, and params, evaluate the
# integral f_{T_i,S_i}(t,s) = \int_0^t f_{T'}(t-u) f_W(s-u) c(F_T',F_W) f_{T_{i-1}}(u) du
# where f_{T_{i-1}}(u) is Gamma(shape=i-1, rate=lambda1)

f_TiSi <- function(t, s, i, lambda1, lambda2, theta, rel.tol = 1e-6) {
  if (s < t) return(0) # impossible since W = s-t >= 0
  if (i == 1) {
    # For i=1, T_{i-1} is degenerate at 0; integral reduces to evaluating at u=0
    tau <- t
    w <- s
    val <- f_exp(tau, lambda1) * f_exp(w, lambda2) * c_clayton(F_exp(tau, lambda1),
                                                               F_exp(w, lambda2),
                                                               theta)
    return(max(val, 1e-300))
  }
  integrand <- function(u) {
    tau <- t - u          # T'_i
    w <- s - u            # W_i
    if (tau < 0 || w < 0) return(0)
    f_tau <- f_exp(tau, lambda1)
    f_w <- f_exp(w, lambda2)
    u1 <- F_exp(tau, lambda1)
    v1 <- F_exp(w, lambda2)
    copd <- c_clayton(u1, v1, theta)
    f_Tim1 <- dgamma(u, shape = i - 1, rate = lambda1)
    return(f_tau * f_w * copd * f_Tim1)
  }
  # numerical integration on [0, t]
  res <- try(integrate(integrand,
                       lower = 0,
                       upper = t,
                       rel.tol = rel.tol,
                       stop.on.error = FALSE), silent = TRUE)
  if (inherits(res, "try-error")) {
    # fallback: coarse grid integration
    xs <- seq(0, t, length.out = 200)
    ys <- sapply(xs, integrand)
    return(max(trapz(xs, ys), 1e-300))
  }
  val <- res$value
  return(max(val, 1e-300))
}

# Helper: simple trapezoidal rule (used if integrate fails)
trapz <- function(x, y) {
  idx <- 2:length(x)
  sum((x[idx] - x[idx - 1]) * (y[idx] + y[idx - 1])) / 2
}

# 4) Log-likelihood
loglik_params <- function(params, data) {
  lambda1 <- params[1]
  lambda2 <- params[2]
  theta <- params[3]
  if (lambda1 <= 0 || lambda2 <= 0 || theta < 0) return(1e20)
  n <- nrow(data)
  ll <- 0
  for (idx in seq_len(n)) {
    t <- data$Ti[idx]
    s <- data$Si[idx]
    i <- data$i[idx]
    val <- f_TiSi(t, s, i, lambda1, lambda2, theta)
    ll <- ll + log(val)
    # early stop if -Inf
    if (!is.finite(ll)) return(1e20)
  }
  return(-ll) # optimizer minimizes
}

#  5) Estimation (MLE) 
estimate_params <- function(data, init = NULL, lower = c(1e-6, 1e-6, 0), upper = c(Inf, Inf, Inf)) {
  if (is.null(init)) {
    lambda1_init <- 1 / mean(pmax(data$Tprime, 1e-6))
    lambda2_init <- 1 / mean(pmax(data$Wi, 1e-6))
    theta_init <- 0.5
    init <- c(lambda1_init, lambda2_init, theta_init)
  }
  opt <- optim(par = init,
               fn = loglik_params,
               data = data,
               method = "L-BFGS-B",
               lower = lower,
               upper = upper,
               control = list(maxit = 200))
  est <- opt$par
  names(est) <- c("lambda1", "lambda2", "theta")
  return(list(est = est, optim = opt))
}
```


```{r}
# 6) Delay probabilities p^{(l)} (numerical) 
# For event k occurring in year j (interval I_j), compute
# p^{(l)}_{k,j} = P(S in I_{j+l} | T in I_j) = P(T in I_j, S in I_{j+l}) / P(T in I_j)
# We compute denominator analytically (Gamma cdf) and numerator by double integration

P_Ti_in_interval <- function(i, interval, lambda1) {
  shape <- i
  # Note: Ti ~ Gamma(shape = i, rate = lambda1)
  lower <- interval[1]
  upper <- interval[2]
  return(pgamma(upper, shape = shape, rate = lambda1) - pgamma(lower, shape = shape, rate = lambda1))
}

P_Ti_Si_in_intervals_num <- function(i, interval_t, interval_s, lambda1, lambda2, theta, grid_t = 30, grid_s = 30) {
  # approximate double integral by grid on t and s
  ts <- seq(interval_t[1], interval_t[2], length.out = grid_t)
  ss <- seq(interval_s[1], interval_s[2], length.out = grid_s)
  vals <- matrix(0, nrow = grid_t, ncol = grid_s)
  for (ii in seq_along(ts)) {
    for (jj in seq_along(ss)) {
      t <- ts[ii]; s <- ss[jj]
      vals[ii, jj] <- f_TiSi(t, s, i, lambda1, lambda2, theta)
    }
  }
  # trapezoidal 2D
  dx <- diff(ts)[1]
  dy <- diff(ss)[1]
  integral <- sum(vals) * dx * dy
  return(integral)
}

# Compute probabilities p^{(l)} for all events and desired lags
compute_delay_probs <- function(data, params, years, max_lag = 5, grid_t = 30, grid_s = 30) {
  lambda1 <- params[1]; lambda2 <- params[2]; theta <- params[3]
  n <- nrow(data)
  # define yearly intervals in days (years vector as numeric years)
  year_bounds <- as.Date(paste0(years, "-01-01"))
  origin <- min(data$AccDate)
  # convert to days since origin
  bounds_days <- as.numeric(difftime(year_bounds, origin, units = "days"))
  # extend with last + 365 for upper bound of last year
  bounds_days <- c(bounds_days, tail(bounds_days, 1) + 365)
  # For each event k and its occurrence year j, compute p for l = 0..max_lag
  probs <- matrix(0, nrow = n, ncol = max_lag + 1)
  for (k in seq_len(n)) {
    Ti_val <- data$Ti[k]
    occ_year <- as.integer(format(data$AccDate[k], "%Y"))
    # find j index
    j_idx <- which(years == occ_year)
    if (length(j_idx) == 0) next
    for (l in 0:max_lag) {
      # intervals in days
      interval_t <- c(bounds_days[j_idx], bounds_days[j_idx + 1])
      if ((j_idx + l) > length(years)) {
        interval_s <- c(bounds_days[length(bounds_days)],
                        bounds_days[length(bounds_days)])
        probs[k, l + 1] <- 0
      } else {
        interval_s <- c(bounds_days[j_idx + l], bounds_days[j_idx + l + 1])
        num <- P_Ti_Si_in_intervals_num(data$i[k],
                                        interval_t, 
                                        interval_s, 
                                        lambda1,
                                        lambda2,
                                        theta,
                                        grid_t = grid_t, 
                                        grid_s = grid_s)
        den <- P_Ti_in_interval(data$i[k], interval_t, lambda1)
        probs[k, l + 1] <- ifelse(den > 0, num / den, 0)
      }
    }
  }
  colnames(probs) <- paste0("lag_", 0:max_lag)
  return(probs)
}

# 7) Build complete reporting triangle 
# Using probs matrix (n x (max_lag+1)) and event-year mapping, aggregate counts
build_triangle_expected <- function(data, probs, years, max_lag = 5) {
  # rows: origin years; cols: development years (0..max_lag)
  origin_years <- years
  n_years <- length(origin_years)
  triangle <- matrix(0, nrow = n_years, ncol = max_lag + 1)
  rownames(triangle) <- as.character(origin_years)
  colnames(triangle) <- paste0("dev_", 0:max_lag)
  for (k in seq_len(nrow(data))) {
    occ_year <- as.integer(format(data$AccDate[k], "%Y"))
    j_idx <- which(origin_years == occ_year)
    if (length(j_idx) == 0) next
    # weight is 1 per claim (if you have monetary amounts, adapt)
    for (l in 0:max_lag) {
      triangle[j_idx, l + 1] <- triangle[j_idx, l + 1] + probs[k, l + 1]
    }
  }
  return(triangle)
}
```

```{r}
prep <- prepare_times(data)
data_p <- prep$data
res_mle <- estimate_params(data_p)
params <- res_mle$est
print(params)
```



































